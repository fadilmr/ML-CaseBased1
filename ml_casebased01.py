# -*- coding: utf-8 -*-
"""ML-CaseBased01

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uW8BnruW8Wc6-VgMKPcxSgeh4kXOpDjm
"""

# import data
!wget https://raw.githubusercontent.com/fadilmr/ML-CaseBased1/main/audit_risk.csv

# Commented out IPython magic to ensure Python compatibility.
# import pandas, numpy, matplotlib, seaborn
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

"""# Dataset Audit_Risk"""

# read data
df = pd.read_csv('audit_risk.csv')
df.head()

df.describe()

df.info()

"""## Preprocessing"""

df = df.drop(['TOTAL'], axis = 1)

"""drop total karena total merupakan jumlah dari para_A dan para_B sehingga tidak terlalu relevan

mencari value kosong dari tiap attribute
"""

df.isnull().sum()

"""mengisi nilai money value dengan mean nya"""

df['Money_Value'].fillna((df['Money_Value'].mean()), inplace = True)

"""mencari nilai korelasi tiap atribut"""

corr = df.corr()
top_corr_features = corr.index
plt.figure(figsize=(20,20))
g=sns.heatmap(df[top_corr_features].corr(),annot=True,cmap="RdYlGn")

"""drop Detection_risk karena tidak memiliki korelasi dengan atribut lain"""

df = df.drop(['Detection_Risk'], axis = 1)

"""mencari nilai object dari LOCATION_ID"""

df['LOCATION_ID'].unique()

"""menghapus instance Location_ID yang bertipe string"""

df = df[(df.LOCATION_ID != 'LOHARU')]
df = df[(df.LOCATION_ID != 'NUH')]
df = df[(df.LOCATION_ID != 'SAFIDON')]
df = df.astype(float)

df = df.drop_duplicates(keep = 'first')
print("Updated number of rows in the dataset: ",len(df))

df.info()

"""## Train"""

data_df = df.drop(['Audit_Risk'], axis=1)
x = data_df.drop(['Risk'], axis = 1)
y = data_df['Risk']

"""Split data menjadi test dan train lalu di normalisasi menggunakan MinMaxScaler"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.25, random_state=0)
scaler = MinMaxScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.fit_transform(x_test)

"""Implementasi PCA untuk mereduksi dimensi"""

from sklearn.decomposition import PCA

pca = PCA(n_components=0.95)

x_train_reduced = pca.fit_transform(x_train)
x_test_reduced = pca.fit_transform(x_test)

"""Membuat Model menggunakan tensorflow dengan 3 Hidden Layer, 1 input Layer dan 1 Output Layer. menggunakan optimizer adam dengan learning rate 0.00005"""

import tensorflow as tf

model = tf.keras.Sequential (
    [
        tf.keras.layers.Dense(128, input_shape = (9, ), activation = 'relu'),
        tf.keras.layers.Dense(64),
        tf.keras.layers.Dense(32),
        tf.keras.layers.Dense(16),
        tf.keras.layers.Dense(1),
    ]
)

model.compile(loss = 'binary_crossentropy', optimizer = tf.optimizers.Adam(learning_rate=0.00005), metrics = ['accuracy'])

"""Penggunaan Callback agar jika target terpenuhi proses training dapat berhenti"""

target = 0.9
class callbacks(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs = None):
        if logs.get('accuracy') >= target:
            print('\nFor Epoch', epoch, '\nAkurasi telah mencapai = %2.2f%%' %(logs['accuracy']*100), 'proses training selesai.')
            self.model.stop_training = True
callback = callbacks()

model.summary()

"""Proses Training"""

history = model.fit(x_train_reduced, y_train, epochs = 1000, batch_size = 100, callbacks=[callback])

import matplotlib.pyplot as plt

plt.plot(history.history['accuracy'])

plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')

plt.legend(['Train', 'Test'], loc='lower right')
plt.show()