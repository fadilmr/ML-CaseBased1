# -*- coding: utf-8 -*-
"""ML-CaseBased01

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uW8BnruW8Wc6-VgMKPcxSgeh4kXOpDjm
"""

# import data
!wget https://raw.githubusercontent.com/fadilmr/ML-CaseBased1/main/audit_risk.csv

# Commented out IPython magic to ensure Python compatibility.
# import pandas, numpy, matplotlib, seaborn
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

"""# Dataset Audit_Risk"""

# read data
df = pd.read_csv('audit_risk.csv')
df.head()

df.describe()

df.info()

"""## Preprocessing

drop total karena total merupakan jumlah dari para_A dan para_B sehingga tidak terlalu relevan
"""

df = df.drop(['TOTAL'], axis = 1)

"""mencari value kosong dari tiap attribute"""

df.isnull().sum()

"""mengisi nilai money value dengan mean nya"""

df['Money_Value'].fillna((df['Money_Value'].mean()), inplace = True)

"""mencari nilai korelasi tiap atribut"""

corr = df.corr()
top_corr_features = corr.index
plt.figure(figsize=(20,20))
g=sns.heatmap(df[top_corr_features].corr(),annot=True,cmap="RdYlGn")

"""drop Detection_risk karena tidak memiliki korelasi dengan atribut lain"""

df = df.drop(['Detection_Risk'], axis = 1)

"""mencari nilai object dari LOCATION_ID"""

df['LOCATION_ID'].unique()

"""menghapus instance Location_ID yang bertipe string"""

df = df[(df.LOCATION_ID != 'LOHARU')]
df = df[(df.LOCATION_ID != 'NUH')]
df = df[(df.LOCATION_ID != 'SAFIDON')]
df = df.astype(float)

df = df.drop_duplicates(keep = 'first')
print("Updated number of rows in the dataset: ",len(df))

df.info()

"""## Train"""

sns.countplot(df['Risk'], label = "Count")

data_df = df.drop(['Audit_Risk'], axis=1)
x = data_df.drop(['Risk'], axis = 1)
y = data_df['Risk']

"""Split data menjadi test dan train lalu di normalisasi menggunakan MinMaxScaler"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2)
x_train_scaled = scaler.fit_transform(x_train)
x_test_scaled = scaler.transform(x_test)

"""Implementasi PCA (Principal Component Analysis) untuk mereduksi dimensi"""

from sklearn.decomposition import PCA

pca = PCA(n_components=0.95)

x_train_reduced = pca.fit_transform(x_train_scaled)
x_test_reduced = pca.transform(x_test_scaled)

"""Membuat Model menggunakan tensorflow dengan 6 Hidden Layer, 1 input Layer dan 1 Output Layer. menggunakan optimizer adam dengan learning rate 0.00005"""

import tensorflow as tf

model = tf.keras.Sequential (
    [
        tf.keras.layers.Dense(128, activation = 'relu'),
        tf.keras.layers.Dense(64),
        tf.keras.layers.Dense(32),
        tf.keras.layers.Dense(16),
        tf.keras.layers.Dense(8),    
        tf.keras.layers.Dense(4),        
        tf.keras.layers.Dense(2),        
        tf.keras.layers.Dense(1, activation = 'sigmoid'),
    ]
)

model.compile(loss = 'binary_crossentropy', optimizer = tf.optimizers.Adam(learning_rate=0.0000005), metrics = ['accuracy', 'Precision', 'Recall'])

"""Penggunaan Callback agar jika target terpenuhi proses training dapat berhenti"""

target = 0.9
class callbacks(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs = None):
        if logs.get('accuracy') >= target:
            print('\nFor Epoch', epoch, '\nAkurasi telah mencapai = %2.2f%%' %(logs['accuracy']*100), 'proses training selesai.')
            self.model.stop_training = True
callback = callbacks()

"""Proses Training"""

history = model.fit(x_train_reduced, y_train, epochs = 1000, callbacks=[callback])

import matplotlib.pyplot as plt

plt.plot(history.history['accuracy'])
plt.plot(history.history['precision'])
plt.plot(history.history['recall'])
plt.plot(history.history['loss'])

plt.title('Evaluation')
plt.ylabel('Value')
plt.xlabel('Epoch')

plt.legend(['accuracy', 'precision', 'recall', 'loss'], loc='lower right')
plt.show()

y_train_pred = model.predict(x_train_reduced)
y_test_pred = model.predict(x_test_reduced)

y_train_pred_class = [1 if prob > 0.5 else 0 for prob in np.ravel(y_train_pred)]
y_test_pred_class = [1 if prob > 0.5 else 0 for prob in np.ravel(y_test_pred)]

y_train_pred_class[:10], y_test_pred_class[:10]

from sklearn.metrics import confusion_matrix

print(confusion_matrix(y_train,y_train_pred_class))
print(confusion_matrix(y_test,y_test_pred_class))

from sklearn.metrics import accuracy_score, precision_score, recall_score
print(f'Accuracy : {accuracy_score(y_test, y_test_pred_class)*100:.2f}%')
print(f'Precision : {precision_score(y_test, y_test_pred_class)*100:.2f}%')
print(f'Recall : {recall_score(y_test, y_test_pred_class)*100:.2f}%')